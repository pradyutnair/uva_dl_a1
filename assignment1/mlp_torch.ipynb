{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T12:55:59.011790Z",
     "start_time": "2024-11-02T12:55:59.009011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "from mlp_pytorch import MLP\n",
    "import cifar10_utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "id": "168719f4256a2bec",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T12:55:59.041071Z",
     "start_time": "2024-11-02T12:55:59.025679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2024 University of Amsterdam\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Deep Learning Course (UvA) | Fall 2024\n",
    "# Date Created: 2024-10-28\n",
    "################################################################################\n",
    "\"\"\"\n",
    "This module implements a multi-layer perceptron (MLP) in PyTorch.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a Multi-layer Perceptron in PyTorch.\n",
    "    It handles the different layers and parameters of the model.\n",
    "    Once initialized an MLP object can perform forward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden, n_classes, use_batch_norm=False):\n",
    "        \"\"\"\n",
    "        Initializes MLP object.\n",
    "\n",
    "        Args:\n",
    "          n_inputs: number of inputs.\n",
    "          n_hidden: list of ints, specifies the number of units\n",
    "                    in each linear layer. If the list is empty, the MLP\n",
    "                    will not have any linear layers, and the model\n",
    "                    will simply perform a multinomial logistic regression.\n",
    "          n_classes: number of classes of the classification problem.\n",
    "                     This number is required in order to specify the\n",
    "                     output dimensions of the MLP\n",
    "          use_batch_norm: If True, add a Batch-Normalization layer in between\n",
    "                          each Linear and ELU layer.\n",
    "\n",
    "        TODO:\n",
    "        Implement module setup of the network.\n",
    "        The linear layer have to initialized according to the Kaiming initialization.\n",
    "        Add the Batch-Normalization _only_ is use_batch_norm is True.\n",
    "\n",
    "        Hint: No softmax layer is needed here. Look at the CrossEntropyLoss module for loss calculation.\n",
    "        \"\"\"\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "\n",
    "        # Initialize the parent class\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # Initialize the layers\n",
    "        self.layers = OrderedDict()\n",
    "\n",
    "        # Add the hidden layers\n",
    "        if len(n_hidden) > 0:\n",
    "            for i, hidden_units in enumerate(n_hidden):\n",
    "                # Linear layer with Kaiming initialization\n",
    "                self.layers[f'Hidden_{i}'] = nn.Linear(n_inputs, hidden_units)\n",
    "                nn.init.kaiming_normal_(self.layers[f'Hidden_{i}'].weight)\n",
    "            \n",
    "                # Batch normalization\n",
    "                if use_batch_norm:\n",
    "                    self.layers[f'BatchNorm_{i}'] = nn.BatchNorm1d(hidden_units)\n",
    "            \n",
    "                # Activation function\n",
    "                self.layers[f'ELU_{i}'] = nn.ELU()\n",
    "            \n",
    "                # Update the number of inputs\n",
    "                n_inputs = hidden_units\n",
    "        \n",
    "        # Add the output layer (no activation function)\n",
    "        self.layers['Output'] = nn.Linear(n_inputs, n_classes)\n",
    "        nn.init.kaiming_normal_(self.layers['Output'].weight)\n",
    "\n",
    "        # Instantiate the model\n",
    "        self.model = nn.Sequential(self.layers)\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Prints the architecture of the network.\n",
    "        \"\"\"\n",
    "        return str(self.model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass of the input. Here an input tensor x is transformed through\n",
    "        several layer transformations.\n",
    "\n",
    "        Args:\n",
    "          x: input to the network\n",
    "        Returns:\n",
    "          out: outputs of the network\n",
    "\n",
    "        TODO:\n",
    "        Implement forward pass of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        out = self.model(x)\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "\n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        Returns the device on which the model is. Can be useful in some situations.\n",
    "        \"\"\"\n",
    "        return next(self.parameters()).device\n",
    "\n"
   ],
   "id": "8e0cfbdf0ab10c3",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:13:10.190052Z",
     "start_time": "2024-11-02T13:13:10.181056Z"
    }
   },
   "cell_type": "code",
   "source": "mlp_torch = MLP(3072, [128], 10, use_batch_norm=False)",
   "id": "44e2113c9609f960",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:13:10.832463Z",
     "start_time": "2024-11-02T13:13:10.828990Z"
    }
   },
   "cell_type": "code",
   "source": "mlp_torch",
   "id": "7e4ad637449cd4e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Sequential(\n",
       "    (Hidden_0): Linear(in_features=3072, out_features=128, bias=True)\n",
       "    (ELU_0): ELU(alpha=1.0)\n",
       "    (Output): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:08:09.024484Z",
     "start_time": "2024-11-02T13:08:07.730793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the random seeds for reproducibility\n",
    "seed = 42\n",
    "data_dir = '../data'\n",
    "batch_size = 128\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set default device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps')\n",
    "\n",
    "# Loading the dataset\n",
    "cifar10 = cifar10_utils.get_cifar10(data_dir)\n",
    "cifar10_loader = cifar10_utils.get_dataloader(cifar10, batch_size=batch_size,\n",
    "                                              return_numpy=False)"
   ],
   "id": "d258452b5689dfea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:09:48.756652Z",
     "start_time": "2024-11-02T13:09:48.750691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy, i.e. the average of correct predictions\n",
    "    of the network.\n",
    "\n",
    "    Args:\n",
    "      predictions: 2D float array of size [batch_size, n_classes], predictions of the model (logits)\n",
    "      labels: 2D int array of size [batch_size, n_classes]\n",
    "              with one-hot encoding. Ground truth labels for\n",
    "              each sample in the batch\n",
    "    Returns:\n",
    "      accuracy: scalar float, the accuracy of predictions,\n",
    "                i.e. the average correct predictions over the whole batch\n",
    "\n",
    "    TODO:\n",
    "    Implement accuracy computation.\n",
    "    \"\"\"\n",
    "\n",
    "    #######################\n",
    "    # PUT YOUR CODE HERE  #\n",
    "    #######################\n",
    "    # Compute predicted classes\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == targets)\n",
    "    #######################\n",
    "    # END OF YOUR CODE    #\n",
    "    #######################\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"\n",
    "    Performs the evaluation of the MLP model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "      model: An instance of 'MLP', the model to evaluate.\n",
    "      data_loader: The data loader of the dataset to evaluate.\n",
    "    Returns:\n",
    "      avg_accuracy: scalar float, the average accuracy of the model on the dataset.\n",
    "\n",
    "    TODO:\n",
    "    Implement evaluation of the MLP model on a given dataset.\n",
    "\n",
    "    Hint: make sure to return the average accuracy of the whole dataset,\n",
    "          independent of batch sizes (not all batches might be the same size).\n",
    "    \"\"\"\n",
    "\n",
    "    #######################\n",
    "    # PUT YOUR CODE HERE  #\n",
    "    #######################\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_data, batch_targets in data_loader:\n",
    "        # Flatten the whole batch\n",
    "        batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "        print(f\"Batch data shape: {batch_data.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(batch_data)\n",
    "\n",
    "        batch_accuracy = accuracy(predictions, batch_targets)\n",
    "        total_correct += batch_accuracy * batch_data.shape[0]\n",
    "        total_samples += batch_data.shape[0]\n",
    "\n",
    "    # Compute average accuracy\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "    #######################\n",
    "    # END OF YOUR CODE    #\n",
    "    #######################\n",
    "\n",
    "    return avg_accuracy"
   ],
   "id": "4c7c7a3c817804e0",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:10:54.160738Z",
     "start_time": "2024-11-02T13:10:50.107543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set default device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loading the dataset\n",
    "cifar10 = cifar10_utils.get_cifar10(data_dir)\n",
    "cifar10_loader = cifar10_utils.get_dataloader(cifar10, batch_size=batch_size,\n",
    "                                              return_numpy=False)\n",
    "\n",
    "#######################\n",
    "# PUT YOUR CODE HERE  #\n",
    "#######################\n",
    "n_inputs = 32 * 32 * 3\n",
    "n_classes = 10\n",
    "hidden_dims = [128]\n",
    "use_batch_norm = True\n",
    "lr = 1e-3\n",
    "epochs = 1\n",
    "\n",
    "# TODO: Initialize model and loss module\n",
    "model = MLP(n_inputs, hidden_dims, n_classes, use_batch_norm).to(device)\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "# TODO: Training loop including validation\n",
    "# TODO: Do optimization with the simple SGD optimizer\n",
    "val_accuracies = []\n",
    "# TODO: Test best model\n",
    "test_accuracy = 0.0\n",
    "# TODO: Add any information you might want to save for plotting\n",
    "logging_dict = {'train_loss': [], 'val_accuracy': [], 'train_epoch_loss': []}\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "best_model = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    for inputs, targets in cifar10_loader['train']:\n",
    "        # Flatten data\n",
    "        inputs = inputs.reshape(inputs.shape[0], -1)\n",
    "        # Move data to device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_module(outputs, targets)\n",
    "        logging_dict['train_loss'].append(loss.item())\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    model.eval()"
   ],
   "id": "eee888c654f50060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf69e6f7e9da4fce9945905836993e85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:12:05.107369Z",
     "start_time": "2024-11-02T13:12:05.083829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in cifar10_loader['validation']:\n",
    "    # Flatten the inputs\n",
    "    inputs = inputs.view(inputs.size(0), -1)\n",
    "    # Move the inputs and targets to the device\n",
    "    print(inputs.shape, targets.shape)\n",
    "    predictions = model(inputs)\n",
    "    print(predictions.shape)\n",
    "    break"
   ],
   "id": "3cc26c8a6b6e1a2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3072]) torch.Size([128])\n",
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:15:50.049927Z",
     "start_time": "2024-11-02T13:15:50.045401Z"
    }
   },
   "cell_type": "code",
   "source": "np.mean(np.argmax(predictions.detach().numpy(), axis=1) == targets.numpy())",
   "id": "7d34fe2e52272c9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2890625"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:15:00.199520Z",
     "start_time": "2024-11-02T13:15:00.195524Z"
    }
   },
   "cell_type": "code",
   "source": "targets",
   "id": "cb6995f20ab5556e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 9, 1, 5, 7, 9, 2, 1, 8, 6, 9, 8, 6, 7, 5, 8, 5, 8, 9, 0, 0, 6, 6, 1,\n",
       "        4, 9, 3, 9, 5, 9, 8, 2, 5, 5, 1, 6, 5, 0, 5, 2, 2, 1, 1, 0, 1, 1, 2, 5,\n",
       "        4, 9, 7, 3, 4, 4, 7, 4, 9, 4, 3, 1, 7, 3, 7, 8, 0, 4, 4, 6, 3, 4, 1, 2,\n",
       "        9, 4, 1, 2, 4, 1, 4, 0, 2, 3, 0, 0, 2, 3, 0, 9, 3, 3, 4, 6, 2, 3, 5, 0,\n",
       "        6, 3, 0, 7, 7, 4, 5, 9, 5, 5, 1, 8, 1, 3, 9, 0, 7, 7, 6, 5, 2, 2, 1, 1,\n",
       "        9, 1, 2, 6, 4, 6, 9, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c439f579407652a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
